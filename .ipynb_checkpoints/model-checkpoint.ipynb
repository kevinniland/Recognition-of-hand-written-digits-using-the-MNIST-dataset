{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of \n",
    "# hardcopy formats and interactive environments across platforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy is the fundamental package for scientific computing with Python\n",
    "import numpy as np\n",
    "\n",
    "# Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano\n",
    "import keras\n",
    "\n",
    "# Import the MNIST dataset directly from the Keras API\n",
    "from keras.datasets import mnist \n",
    "\n",
    "# The Sequential model is a linear stack of layers\n",
    "from keras.models import Sequential \n",
    "\n",
    "#''' Please refer to the wiki of the repository from which this project is located for a more in-depth analysis and explanation\n",
    "   # of various imports, packages, the model, how it is trained, etc. '''\n",
    "# Dense - implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise \n",
    "# activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias \n",
    "# vector created by the layer (only applicable if use_bias is True)\n",
    "\n",
    "# Dropout - applies Dropout to the input. This consists of randomly setting a fraction rate of input units to 0 \n",
    "# at each update during training time, which helps prevent overfitting\n",
    "\n",
    "# Flatten - 'flattens' the input. Does not affect the batch size. Flattening a tensor means to remove all of the dimensions except for one\n",
    "# Conv2D - 2D convolution layer (e.g. spatial convolution over images). Creates a convolution kernel that is convolved with the layer input \n",
    "# to produce a tensor of outputs\n",
    "\n",
    "# MaxPooling2D - Max pooling operation for spatial data\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# Allows for the saving and loading of the model\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras import backend as kBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the MNIST data set directly from the TensorFlow and Keras API. The MNIST dataset contains 60,000 training images\n",
    "# and 10,000 testing images (with accompanying labels). We separate these two groups into train_imgs and train_labels for the training\n",
    "# images and training labels, respectively, and test_imgs and test_labels for the test images and test labels respectively\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the size of the image as 28 * 28 pixels\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "#\n",
    "# To be able to use the MNIST dataset with the Keras API, we need to change our array (which is 3-dimensional)\n",
    "# to 4-dimensional numpy arrays\n",
    "\n",
    "# https://stackoverflow.com/questions/49057149/expected-conv2d-1-input-to-have-shape-28-28-1-but-got-array-with-shape-1-2\n",
    "if kBackend.image_data_format() == 'channels_first':\n",
    "    train_imgs = train_imgs.reshape(train_imgs.shape[0], 1, img_rows, img_cols)\n",
    "    test_imgs = test_imgs.reshape(test_imgs.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    train_imgs = train_imgs.reshape(train_imgs.shape[0], img_rows, img_cols, 1)\n",
    "    test_imgs = test_imgs.reshape(test_imgs.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the values of train_imgs and test_imgs are float. This is done so as we can get decimal points after division\n",
    "train_imgs = train_imgs.astype('float32')\n",
    "test_imgs = test_imgs.astype('float32')\n",
    "\n",
    "# We also must 'normalize' our data, as is always required in neural networks.\n",
    "# This can be done by dividing the RGB codes of the images to 255\n",
    "train_imgs /= 255\n",
    "test_imgs /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the model, we can experiment with any number for the first Dense layer. However, the final Dense layer must have\n",
    "# 10 neurons since there are 10 number classes (0, 1, 2, 3, ..., 9)\n",
    "num_classes = 10\n",
    "\n",
    "# \n",
    "rate = 0.5\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model and add the layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())  # Flattens the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 48s 801us/step - loss: 0.2611 - accuracy: 0.9192 - val_loss: 0.0583 - val_accuracy: 0.9820\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 48s 807us/step - loss: 0.0881 - accuracy: 0.9743 - val_loss: 0.0450 - val_accuracy: 0.9847\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 49s 812us/step - loss: 0.0662 - accuracy: 0.9804 - val_loss: 0.0352 - val_accuracy: 0.9878\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 49s 810us/step - loss: 0.0538 - accuracy: 0.9840 - val_loss: 0.0353 - val_accuracy: 0.9874\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 49s 811us/step - loss: 0.0471 - accuracy: 0.9858 - val_loss: 0.0318 - val_accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 49s 810us/step - loss: 0.0412 - accuracy: 0.9875 - val_loss: 0.0312 - val_accuracy: 0.9892\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 49s 810us/step - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0286 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 48s 808us/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 0.0292 - val_accuracy: 0.9908\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 49s 809us/step - loss: 0.0309 - accuracy: 0.9908 - val_loss: 0.0241 - val_accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 48s 806us/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0255 - val_accuracy: 0.9917\n",
      "Saved model. Model will now be loaded on next run through\n"
     ]
    }
   ],
   "source": [
    "# Determines the number of samples that will be propagated through the neural network\n",
    "batch_size = 128\n",
    "\n",
    "# In the context of a neural network, one epoch is the equivalent of one forward pass and one backward pass of\n",
    "# all the training examples\n",
    "\n",
    "# With this model, the model seems to peak (in terms of accuracy) at 20 epochs. Any more epochs after this is more or less overkill\n",
    "num_epoch = 10\n",
    "\n",
    "# To avoid having to train the model each time the program is ran, the trained model can be loaded from a file\n",
    "# If no file has been created, then the model is trained and then saved to a file. For the purposes of this notebook, \n",
    "# the model will be trained each time\n",
    "\n",
    "# try:\n",
    "#     print(\"Model loaded successfully\")\n",
    "#     model = load_model(\"model.h5\")\n",
    "# except:\n",
    "#     print(\"Failed to load model. Creating new model...\")\n",
    "model_log = model.fit(train_imgs, train_labels,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=num_epoch,\n",
    "                          verbose=1,\n",
    "                          validation_data=(test_imgs, test_labels))\n",
    "\n",
    "model.save_weights(\"model.h5\")\n",
    "\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model. Model will now be loaded on next run through\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.8598394e-10 3.0229991e-10 2.3736584e-09 1.5166124e-11 2.5477650e-08\n",
      "  5.0896073e-08 9.9999988e-01 2.2509323e-14 3.6671077e-09 2.4259527e-12]] \n",
      "Predicted number:  6\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(test_imgs[9999].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(model.predict(test_imgs[9999:10000]), \"\\nPredicted number: \", np.argmax(model.predict(test_imgs[9999:10000])))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
